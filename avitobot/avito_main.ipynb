{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.ERROR)\n",
    "#logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"selenium\").setLevel(logging.CRITICAL)\n",
    "\n",
    "from src.pars_tools import ProxyGet, AvitoBot \n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import csv\n",
    "from time import sleep\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser('arguments for setting driver and additional parsing options')\n",
    "    parser.add_argument('--driver', type=str, default='Chrome')\n",
    "    # headless mode:\n",
    "    parser.add_argument('--headless', type=bool, default=True)  \n",
    "    # choose category for parsing: \n",
    "    #    'predlozheniya_uslug'\n",
    "    #    'nedvizhimost'\n",
    "    parser.add_argument('--category', type=str, default='predlozheniya_uslug')  \n",
    "    # pass main url for query: \n",
    "    #    'https://www.avito.ru/moskovskaya_oblast/kvartiry/'\n",
    "    #    'https://www.avito.ru/moskovskaya_oblast/komnaty/'\n",
    "    #    'https://www.avito.ru/moskva/predlozheniya_uslug/transport_perevozki'\n",
    "    parser.add_argument('--url', type=str, default='https://www.avito.ru/moskva/predlozheniya_uslug/transport_perevozki') \n",
    "    # choose category subtype --> int: \n",
    "    # if nedvizhimost is selected then:        1 - sobstvennik,  2 - agentstvo\n",
    "    # if predlozheniya_uslug is selected then: 1 - chastnoe,     2 - companya\n",
    "    parser.add_argument('--usertype', type=int, default=1) \n",
    "    # define query type: 'sdam' for category=nedvizhimost or empty string ''\n",
    "    parser.add_argument('--query', type=str, default='')     \n",
    "    # choose the way to grab each page: soup=>True or selenium=>False\n",
    "    parser.add_argument('--get_wall_soup', type=bool, default=True) \n",
    "    parser.add_argument('--adv_scrap_soup', type=bool, default=True)\n",
    "    parser.add_argument('--findnewadvs', type=dict, default={'findnewadvs':False,'daysback':12})\n",
    "    parser.add_argument('--useproxy', type=bool, default=False)\n",
    "    parser.add_argument('--usesocks', type=bool, default=False)\n",
    "    # pass proxy to selenium driver\n",
    "    parser.add_argument('--proxylst', nargs='+', default=ProxyGet().get_random_proxy()[1]) \n",
    "    # pass proxy to request.get() method\n",
    "    parser.add_argument('--proxyDict', type=dict, default=ProxyGet().get_random_proxy()[0]) \n",
    "    parser.add_argument('--takescreenshot', type=bool, default=False)\n",
    "    # parse mobile=>True/web=>False version of Avito\n",
    "    parser.add_argument('--parsemobile', type=bool, default=True)\n",
    "    # run bot in parallel mode\n",
    "    parser.add_argument('--runparallel', type=bool, default=True) \n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "args = parser.parse_args(argv)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiane child of AvitoBot class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(150):\n",
    "    args.proxylst  = ProxyGet().get_random_proxy()[1]\n",
    "    args.proxyDict = ProxyGet().get_random_proxy()[0]\n",
    "bot = AvitoBot(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bot.driver.get('https://www.whatsmyip.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ProxyGet().collect_proxies()\n",
    "bot.collect_proxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiated object has been deleted\n"
     ]
    }
   ],
   "source": [
    "del bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab number of pages to be parsed and find all advs links on each page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outjson = []\n",
    "offset = 1000\n",
    "prices = [(max(int(i*offset),1),int((i+1)*offset)) for i in range(20)]\n",
    "for items in prices:\n",
    "    pricemin, pricemax = items\n",
    "    print(pricemin, pricemax)\n",
    "    res = bot.advert_collect_by_pages(args.url, pricemax, pricemin)\n",
    "    if res is not None:\n",
    "        outjson.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__len=0\n",
    "for item in outjson:\n",
    "    for k,v in item.items():\n",
    "        __len+=len(item[k])\n",
    "__len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save json file with information about avito advs and relevant links for future parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\anthony\\\\Documents\\\\Python Scripts\\\\avito_parse\\\\avito_links_all_moskovskaya_oblast_kvartiry_agentstva.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(outjson,Path.joinpath(Path(os.getcwd()), 'avito_links_all_moskovskaya_oblast_kvartiry_agentstva.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously saved json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outjson_obsol = joblib.load(Path.joinpath(Path(os.getcwd()), 'avito_links_all_sankt-peterburg_kvartiry_sobstvennik.json'))\n",
    "outjson = outjson_obsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert collected links into mobile-like format if we're going to parse mobile version of Avito.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# outjson = outjson_obsol\n",
    "for item in outjson:\n",
    "    for p,v in tqdm_notebook(item.items()):\n",
    "        for adv in v:\n",
    "            adv['href']=re.sub('www','m',adv['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parsing without parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedpages = [[(k,v) for k,v in item.items()] for item in outjson if len(item)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#total_pages = len(outjson)\n",
    "# for page in tqdm_notebook(range(1,total_pages)):\n",
    "for batches in parsedpages:\n",
    "    for page in tqdm_notebook(batches): \n",
    "        args.proxylst  = ProxyGet().get_random_proxy()[1]\n",
    "        args.proxyDict = ProxyGet().get_random_proxy()[0]\n",
    "        bot = AvitoBot(args)             \n",
    "        res = bot.navigate(page[1])\n",
    "        del bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parsing with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedpages = [[(k,v) for k,v in item.items()] for item in outjson if len(item)!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Parallel with map-delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, JoinableQueue\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver(q,category):\n",
    "    file_path      = Path.joinpath(Path(os.getcwd()), 'csv','avito_db_{}.csv'.format(category))\n",
    "    file_path_pgs  = Path.joinpath(Path(os.getcwd()), 'csv','parsed_pages.dat')\n",
    "    if category == 'nedvizhimost': \n",
    "        headers = ['href', 'title', 'full_text', 'phone', 'region', 'city', 'real_estate', 'type', 'marketplace']\n",
    "    elif category == 'predlozheniya_uslug': \n",
    "        headers = ['href', 'title', 'full_text', 'phone', 'region', 'city', 'uslugi', 'type', 'marketplace']\n",
    "    #if not os.path.isfile(str(file_path)):\n",
    "    with open(file_path, 'a', encoding='utf8') as outcsv:\n",
    "        writer = csv.writer(outcsv, delimiter=',', quotechar='\"', \n",
    "                            quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "#         writer.writerow(['href', 'title', 'full_text', 'phonestr', 'loctext', 'sellerinfo']) \n",
    "#         if not os.path.isfile(str(file_spath)):\n",
    "#             writer.writerow(headers)\n",
    "        file_is_empty = os.stat(str(file_path)).st_size == 0\n",
    "        if file_is_empty:\n",
    "            writer.writerow(headers)     \n",
    "        while True:\n",
    "            strfrom_q = q.get()\n",
    "            if strfrom_q is None: break\n",
    "            indx_batch, page, arrstr = strfrom_q.split('&&&')\n",
    "            val = json.loads(arrstr)                    \n",
    "            for item in val:\n",
    "                writer.writerow(item)                    \n",
    "            with open(file_path_pgs, 'a', encoding='utf8') as f:\n",
    "                f.write(indx_batch + ',' + page + '\\n')                \n",
    "            q.task_done()\n",
    "        # Finish up\n",
    "        q.task_done()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(q, indx_batch, pagesarr):\n",
    "    #q,pagesarr = arg\n",
    "    #collectres = []\n",
    "    #parsedpages[0][0][1]\n",
    "    for page in pagesarr:  \n",
    "#         print('page num#{}'.format(page))\n",
    "        for i in range(20):\n",
    "            args.proxylst  = ProxyGet().get_random_proxy()[1]\n",
    "            args.proxyDict = ProxyGet().get_random_proxy()[0]\n",
    "        bot = AvitoBot(args)             \n",
    "        res = bot.navigate(page[1])\n",
    "        restr = json.dumps(res)\n",
    "        q.put(str(indx_batch) + '&&&' + str(page[0]) + '&&&' + restr)\n",
    "        del bot\n",
    "    #return collectres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using JoinableQueue with Threadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared queue with automated Threads initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(len(parsedpages))\n",
    "num_partitions=1\n",
    "batches = np.array_split(arr, num_partitions)\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(6,-1,-1):\n",
    "#     parsedpages[0].pop(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for indx in tqdm_notebook(batches):\n",
    "    result_queue = JoinableQueue() #Queue()\n",
    "    category=args.category\n",
    "    p = Thread(target=saver, args=(result_queue,category))    \n",
    "    threadlst=[]\n",
    "    p.start()\n",
    "    # We create list of threads and pass shared queue to all of them.\n",
    "    threadlst=[Thread(target=parse_page, args=(result_queue, i, parsedpages[i])) for i in indx]\n",
    "    # Starting threads...\n",
    "    print(\"Start: %s\" % time.ctime())\n",
    "    for th in threadlst:\n",
    "        th.start()\n",
    "    # Waiting for threads to finish execution... \n",
    "    for th in threadlst:\n",
    "        th.join() \n",
    "    print(\"End:   %s\" % time.ctime())\n",
    "\n",
    "    result_queue.put(None) # Poison pill\n",
    "    p.join()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared queue with manual Threads initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_queue = JoinableQueue() #Queue()\n",
    "p = Thread(target=saver, args=(result_queue,))\n",
    "p.start()\n",
    "# We create two threads and pass shared queue to both of them.\n",
    "t0 = Thread(target=parse_page, args=(result_queue, parsedpages[0][:1]))\n",
    "#t1 = Thread(target=parse_page, args=(result_queue, parsedpages[1][:1]))\n",
    "#t2 = Thread(target=parse_page, args=(result_queue, parsedpages[2]))\n",
    "#t3 = Thread(target=parse_page, args=(result_queue, df_split[3]))\n",
    "\n",
    "# Starting threads...\n",
    "print(\"Start: %s\" % time.ctime())\n",
    "t0.start()\n",
    "#t1.start()\n",
    "#t2.start()\n",
    "#t3.start()\n",
    "\n",
    "# Waiting for threads to finish execution...\n",
    "t0.join()\n",
    "#t1.join()\n",
    "#t2.join()\n",
    "#t3.join()\n",
    "print(\"End:   %s\" % time.ctime())\n",
    "\n",
    "# After threads are done, we can read results from the queue.\n",
    "# while not result_queue.empty():\n",
    "#     result = result_queue.get()\n",
    "#     print(result)\n",
    "\n",
    "result_queue.put(None) # Poison pill\n",
    "p.join()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
