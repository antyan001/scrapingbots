{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anthony\\Anaconda3\\lib\\site-packages\\dateparser\\date.py:320: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if not isinstance(languages, (list, tuple, collections.Set)) and languages is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2]), array([3, 4]), array([5, 6])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Tue Mar 19 21:19:31 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/52 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|                                                                                           | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "  2%|█▌                                                                                 | 1/55 [00:05<04:40,  5.19s/it]\n",
      "  2%|█▌                                                                                 | 1/52 [00:05<04:28,  5.27s/it]\n",
      "\n",
      "\n",
      "  2%|█▌                                                                                 | 1/55 [00:05<04:52,  5.42s/it]\n",
      "  4%|███▏                                                                               | 2/52 [00:09<04:10,  5.02s/it]\n",
      "\n",
      "  4%|███                                                                                | 2/55 [00:09<04:26,  5.03s/it]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.ERROR)\n",
    "#logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"selenium\").setLevel(logging.CRITICAL)\n",
    "\n",
    "from src.pars_tools import ProxyGet, AvitoBot \n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import csv\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "from multiprocessing import Process, JoinableQueue\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "def saver(q):\n",
    "    file_path      = Path.joinpath(Path(os.getcwd()), 'csv','avito_db.csv')\n",
    "    file_path_pgs  = Path.joinpath(Path(os.getcwd()), 'csv','parsed_pages.dat')\n",
    "    headers = ['href', 'title', 'full_text', 'phone', 'region', 'city', 'real_estate', 'type', 'marketplace']\n",
    "    #if not os.path.isfile(str(file_path)):\n",
    "    with open(file_path, 'a', encoding='utf8') as outcsv:\n",
    "        writer = csv.writer(outcsv, delimiter=',', quotechar='\"', \n",
    "                            quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "#         writer.writerow(['href', 'title', 'full_text', 'phonestr', 'loctext', 'sellerinfo']) \n",
    "#         if not os.path.isfile(str(file_spath)):\n",
    "#             writer.writerow(headers)\n",
    "        file_is_empty = os.stat(str(file_path)).st_size == 0\n",
    "        if file_is_empty:\n",
    "            writer.writerow(headers)     \n",
    "        while True:\n",
    "            strfrom_q = q.get()\n",
    "            if strfrom_q is None: break\n",
    "            page, arrstr = strfrom_q.split('&&&')\n",
    "            val = json.loads(arrstr)                    \n",
    "            for item in val:\n",
    "                writer.writerow(item)                    \n",
    "            with open(file_path_pgs, 'a', encoding='utf8') as f:\n",
    "                f.write(page + '\\n')                \n",
    "            q.task_done()\n",
    "        # Finish up\n",
    "        q.task_done()   \n",
    "\n",
    "def parse_page(q,pagesarr):\n",
    "    #q,pagesarr = arg\n",
    "    #collectres = []\n",
    "    #parsedpages[0][0][1]\n",
    "    for page in pagesarr:  \n",
    "#         print('page num#{}'.format(page))\n",
    "        try:\n",
    "            args.proxylst  = ProxyGet().get_random_proxy()[1]\n",
    "            args.proxyDict = ProxyGet().get_random_proxy()[0]\n",
    "            bot = AvitoBot(args)             \n",
    "            res = bot.navigate(page[1])\n",
    "            restr = json.dumps(res)\n",
    "            q.put(str(page[0]) + '&&&' + restr)\n",
    "        except:\n",
    "            sleep(7)\n",
    "            bot.closedriver()\n",
    "            bot.restartdriver()\n",
    "            res = bot.navigate(page[1])\n",
    "            restr = json.dumps(res)\n",
    "            q.put(str(page[0]) + '&&&' + restr)            \n",
    "        #collectres+=res \n",
    "        del bot\n",
    "    #return collectres        \n",
    "        \n",
    "\n",
    "parser = argparse.ArgumentParser('arguments for setting driver and additional parsing options')\n",
    "parser.add_argument('--driver', type=str, default='Chrome')\n",
    "parser.add_argument('--headless', type=bool, default=True)   # headless mode\n",
    "parser.add_argument('--url', type=str, default='https://www.avito.ru/sankt-peterburg/kvartiry/')  # pass main url for query: 'https://www.avito.ru/moskovskaya_oblast/kvartiry/'\n",
    "parser.add_argument('--usertype', type=int, default=2)  # choose category: 1 - sobstvennik/private, 2 - agentstvo\n",
    "parser.add_argument('--get_wall_soup', type=bool, default=True) # choose the way to grab each page: soup=>True or selenium=>False\n",
    "parser.add_argument('--adv_scrap_soup', type=bool, default=True)\n",
    "parser.add_argument('--findnewadvs', type=dict, default={'findnewadvs':False,'daysback':4})\n",
    "parser.add_argument('--useproxy', type=bool, default=True)\n",
    "parser.add_argument('--usesocks', type=bool, default=False)\n",
    "parser.add_argument('--proxylst', nargs='+', default=ProxyGet().get_random_proxy()[1]) # pass proxy to selenium driver\n",
    "parser.add_argument('--proxyDict', type=dict, default=ProxyGet().get_random_proxy()[0]) # pass proxy to request.get() method\n",
    "parser.add_argument('--takescreenshot', type=bool, default=False)\n",
    "parser.add_argument('--parsemobile', type=bool, default=True)  # parse mobile=>True/web=>False version of Avito\n",
    "parser.add_argument('--runparallel', type=bool, default=True)  # run bot in parallel mode\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "args = parser.parse_args(argv)\n",
    "\n",
    "outjson = joblib.load(Path.joinpath(Path(os.getcwd()), 'avito_links_all_sankt-peterburg_kvartiry_agentstva.json'))\n",
    "\n",
    "for item in outjson:\n",
    "    for p,v in item.items():\n",
    "        for adv in v:\n",
    "            adv['href']=re.sub('www','m',adv['href'])\n",
    "    \n",
    "parsedpages = [[(k,v) for k,v in item.items()] for item in outjson if len(item)!=0]\n",
    "\n",
    "\n",
    "arr = np.arange(len(parsedpages))\n",
    "num_partitions=3\n",
    "batches = np.array_split(arr, num_partitions)\n",
    "\n",
    "print('batches to be processed:')\n",
    "print(batches)\n",
    "\n",
    "for indx in tqdm(batches):\n",
    "    result_queue = JoinableQueue() #Queue()\n",
    "    p = Thread(target=saver, args=(result_queue,))    \n",
    "    threadlst=[]\n",
    "    p.start()\n",
    "    # We create list of threads and pass shared queue to all of them.\n",
    "    threadlst=[Thread(target=parse_page, args=(result_queue, parsedpages[i])) for i in indx]\n",
    "    # Starting threads...\n",
    "    print(\"Start: %s\" % time.ctime())\n",
    "    for th in threadlst:\n",
    "        th.start()\n",
    "    # Waiting for threads to finish execution...\n",
    "    for th in threadlst:\n",
    "        th.join() \n",
    "    print(\"End:   %s\" % time.ctime())\n",
    "\n",
    "    result_queue.put(None) # Poison pill\n",
    "    p.join()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
